# LLaMA.cpp

Solusi with Ollama supports LLaMA.cpp API endpoints. You can use any LLaMA.cpp API endpoint with Solusi with Ollama.

## Adding LLaMA.cpp API

1. Click on the Solusi with Ollama icon on the browser toolbar.

2. Click on the `Settings` icon.

3. Go to the `OpenAI Compatible API` tab.

4. Click on the `Add Provider` button.

5. Select `LLaMA.cpp` from the dropdown.

6. Enter the `LLaMA.cpp URL`. (by default it is `http://localhost:8080/v1`)

7. Click on the `Save` button.


::: info
You don't need to add any models since Solusi with Ollama will automatically fetch them from the LLaMA.cpp instance you have configured.

The model must be loaded in LLaMA.cpp before Solusi with Ollama can fetch it.
:::